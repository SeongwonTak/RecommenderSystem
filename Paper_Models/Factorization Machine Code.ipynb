{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eed9a9d",
   "metadata": {},
   "source": [
    "# Factorization Machine 실습코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e77028",
   "metadata": {},
   "source": [
    "Factorization Machine이 어떻게 이뤄지는지 실제 실습 코드를 바탕으로 구체적으로 이해해보려고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df4e30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b006a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('./ratings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b5e24",
   "metadata": {},
   "source": [
    "간단한 실습을 위해, movie-lens 가장 최신의 100k를 활용해보고자 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e64e5",
   "metadata": {},
   "source": [
    "## Example. Pytorch를 활용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79719a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>610</td>\n",
       "      <td>166534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1493848402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>610</td>\n",
       "      <td>168248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493850091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>610</td>\n",
       "      <td>168250</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1494273047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>610</td>\n",
       "      <td>168252</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1493846352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>610</td>\n",
       "      <td>170875</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1493846415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating   timestamp\n",
       "0            1        1     4.0   964982703\n",
       "1            1        3     4.0   964981247\n",
       "2            1        6     4.0   964982224\n",
       "3            1       47     5.0   964983815\n",
       "4            1       50     5.0   964982931\n",
       "...        ...      ...     ...         ...\n",
       "100831     610   166534     4.0  1493848402\n",
       "100832     610   168248     5.0  1493850091\n",
       "100833     610   168250     5.0  1494273047\n",
       "100834     610   168252     5.0  1493846352\n",
       "100835     610   170875     3.0  1493846415\n",
       "\n",
       "[100836 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c5fed",
   "metadata": {},
   "source": [
    "Pytorch 활용을 위해서는 custom dataset을 만드는 것이 필요할 것이다.\n",
    "10만개 정도의 데이터 셋 중 80%는 train set, 10%는 valid set, 10%는 test set으로 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60c15649",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLens100kDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        data = ratings.to_numpy()[:,:3]\n",
    "        self.items = data[:,:2]\n",
    "        self.targets = data[:,2]\n",
    "        self.field_dims = np.max(self.items, axis = 0)+1\n",
    "        self.user_field_idx = np.array((0,), dtype = np.long)\n",
    "        self.item_field_idx = np.array((1,), dtype = np.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.targets.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.items[index], self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12e85503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-c1bf7a28f884>:7: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.user_field_idx = np.array((0,), dtype = np.long)\n",
      "<ipython-input-9-c1bf7a28f884>:8: DeprecationWarning: `np.long` is a deprecated alias for `np.compat.long`. To silence this warning, use `np.compat.long` by itself. In the likely event your code does not need to work on Python 2 you can use the builtin `int` for which `np.compat.long` is itself an alias. Doing this will not modify any behaviour and is safe. When replacing `np.long`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.item_field_idx = np.array((1,), dtype = np.long)\n"
     ]
    }
   ],
   "source": [
    "dataset = MovieLens100kDataset()\n",
    "train_len = int(len(dataset) * 0.8)\n",
    "valid_len = int(len(dataset) * 0.1)\n",
    "test_len = len(dataset) - train_len - valid_len\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(dataset, (train_len, valid_len, test_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a0c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a6d437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_set, batch_size = batch_size)\n",
    "valid_loader = DataLoader(valid_set, batch_size = batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d4193",
   "metadata": {},
   "source": [
    "### Model 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946c9568",
   "metadata": {},
   "source": [
    "model 구현은 다음을 바탕으로 진행한다.\n",
    "https://github.com/rixwew/pytorch-fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a6c3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesLinear(torch.nn.Module):\n",
    "    def __init__(self, field_dims, output_dim = 1):\n",
    "        super().__init__()\n",
    "        # 입력 차원 임베딩.\n",
    "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "        # 출력시의 편향 학습\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(output_dim,))\n",
    "        # offset의, 역할은 무엇인가?\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype = np.long)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return torch.sum(self.fc(x), dim = 1) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1640b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesEmbedding(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype = np.long)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a08abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine(torch.nn.Module):\n",
    "    def __init__(self, reduce_sum = True):\n",
    "        super().__init__()\n",
    "        self.reduce_sum = reduce_sum\n",
    "        \n",
    "    def forward(self, x):\n",
    "        square_of_sum = torch.sum(x, dim = 1) ** 2\n",
    "        sum_of_square = torch.sum(x ** 2, dim = 1)\n",
    "        ix = square_of_sum - sum_of_square\n",
    "        if self.reduce_sum:\n",
    "            ix = torch.sum(ix, dim = 1, keepim = True)\n",
    "        return 0.5 * ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cac2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachineModel(torch.nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.linear = FeaturesLinear(field_dims)\n",
    "        self.fm = FactorizaionMachine(reduce_sum = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x) + self.fm(self.embedding(x))\n",
    "        return torch.sigmoid(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, data_loader, criterion, device, log_interval=100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    tk0 = tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (fields, target) in enumerate(tk0):\n",
    "        fields, target = fields.to(device), target.to(device)\n",
    "        y = model(fields)\n",
    "        loss = criterion(y, target.float())\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            tk0.set_postfix(loss=total_loss / log_interval)\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    targets, predicts = list(), list()\n",
    "    with torch.no_grad():\n",
    "        for fields, target in tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0):\n",
    "            fields, target = fields.to(device), target.to(device)\n",
    "            y = model(fields)\n",
    "            targets.extend(target.tolist())\n",
    "            predicts.extend(y.tolist())\n",
    "    return roc_auc_score(targets, predicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(model_name, dataset).to(device)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    early_stopper = EarlyStopper(num_trials=2, save_path=f'{save_dir}/{model_name}.pt')\n",
    "    for epoch_i in range(epoch):\n",
    "        train(model, optimizer, train_data_loader, criterion, device)\n",
    "        auc = test(model, valid_data_loader, device)\n",
    "        print('epoch:', epoch_i, 'validation: auc:', auc)\n",
    "        if not early_stopper.is_continuable(model, auc):\n",
    "            print(f'validation: best auc: {early_stopper.best_accuracy}')\n",
    "            break\n",
    "    auc = test(model, test_data_loader, device)\n",
    "    print(f'test auc: {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e01e4fc",
   "metadata": {},
   "source": [
    "## Example. xlearn을 활용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ae495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3bf48da",
   "metadata": {},
   "source": [
    "## Example. FastFM을 활용하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0cfe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49672715",
   "metadata": {},
   "source": [
    "## 더 나아가는 방법\n",
    "Factorization Machine 이상의 방법론에 대해 생각해 볼 수 있을것이다.\n",
    "DeepFM 등의 방법론도 존재하고, Factorization Machine을 개선한 모델에 대해서는 차차 알아가고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd4cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
